{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from urllib.parse import quote_plus\n",
    "from urllib.request import urlopen\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "def Driver():\n",
    "    try:\n",
    "        path = chromedriver_autoinstaller.install()\n",
    "        \n",
    "    except FileNotFoundError as err:\n",
    "        print(\"크롬 브라우저를 찾을 수 없습니다. 설치 후 재시도 하시기 바랍니다.\")\n",
    "        webbrowser.open(\"https://www.google.com/intl/ko/chrome/\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    #options.add_argument(\"headless\")\n",
    "    options.add_argument('--start-fullscreen')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    wd = webdriver.Chrome(options = options)\n",
    "    return wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 URL과 상업적 이용 가능 옵션\n",
    "def make_url(word):\n",
    "    base_url = 'https://unsplash.com/ko/s/%EC%82%AC%EC%A7%84/'\n",
    "    return base_url + word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selenium과 bs4로 element로 이미지의 url를 추출하려했으나, 웹사이트 보안 정책인지 뭔지는 몰라도 element를 찾을 수가 없어서 추출이 불가 했다.\n",
    "그런 이유로 unplash에서 권장하는 정책으로 Rest API 요쳥을 통해서 image URL을 추출했다. 더욱 빠르기도하고 방법도 간단했다.\n",
    "다만 demo버전의 권한으로는 시간당 50번의 요청밖에 하지 못하고, 페이지당 최대 30개의 이미지를 추출할 수 있어서 최대 50 x 30 시간 당 최대 1500개 까지만 크롤링이 가능하다는 단점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45번째 페이지에서 요청이 거부되었습니다. 45번째 페이지 부터 다시 시작해주세요! Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "search_word = 'asian'\n",
    "images = []\n",
    "\n",
    "\n",
    "# images['results'][0]['urls']['regular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1319"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당하는 폴더가 없을 경우 생성해주는 함수\n",
    "def makedirs(path): \n",
    "   try: \n",
    "        os.makedirs(path) \n",
    "   except OSError: \n",
    "       if not os.path.isdir(path): \n",
    "           raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler:\n",
    "    def __init__(self, query, start_page=1):\n",
    "        self.query = query\n",
    "        self.start_page = start_page\n",
    "        self.images = []\n",
    "    \n",
    "    def activate(self):\n",
    "        \n",
    "        res = requests.get(f'https://api.unsplash.com/search/photos/?client_id=your_access_key&query={search_word}&page={i}&per_page=30')\n",
    "        response = res.json()\n",
    "        total_page = response['total_pages']\n",
    "        while self.start_page <= total_page:\n",
    "            for i in range(self.start_page,total_page+1):\n",
    "                try:                                      \n",
    "                    for element in response['results']:\n",
    "                        self.images.append(element['urls']['regular'])   \n",
    "                except Exception as e:\n",
    "                    print(f\"{i}번째 페이지에서 요청이 거부되었습니다. 한 시간 뒤에 {i}번째 페이지 부터 다시 시작 합니다!\", e)\n",
    "                    break\n",
    "                self.start_page = i\n",
    "                time.sleep(60*60+1)       \n",
    "        print(f'추출한 image url의 개수는 {len(self.images)}입니다.')\n",
    "    \n",
    "    def execute_images(self):\n",
    "        def save_images(image_url, paths, i):\n",
    "            import base64\n",
    "            import os\n",
    "                \n",
    "            if 'data:' in str(image_url):\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    t= urlopen(image_url).read()\n",
    "                    file = open(os.path.join(paths, str(i)+\".gif\"), 'wb')\n",
    "                    file.write(t)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        for i, image in enumerate(self.images,1):\n",
    "            base_path = os.path.abspath(os.getcwd())\n",
    "            save_path = base_path + f'/{self.query}'\n",
    "            makedirs(save_path)\n",
    "            save_images(image, save_path, str(i), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mgd81\\\\github\\\\Crawlers_py'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 디렉토리 위치\n",
    "import os\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
